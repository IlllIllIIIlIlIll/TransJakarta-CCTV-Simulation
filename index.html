<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover" />
  <title>CrowdEase ‚Äî Live Person Counter</title>
  <style>
    :root { --bg:#0b0c10; --fg:#eaf0f6; --muted:#99a3ad; --accent:#27c4a0; --err:#ff5a5f; }
    html,body{margin:0;background:var(--bg);color:var(--fg);font:16px/1.4 system-ui,Segoe UI,Roboto,Helvetica,Arial}
    .wrap{max-width:1080px;margin:auto;padding:16px}
    h1{font-size:1.2rem;margin:0 0 12px}
    .toolbar{display:flex;gap:8px;flex-wrap:wrap;align-items:center;margin-bottom:12px}
    .toolbar .sp{flex:1}
    button{border:none;border-radius:10px;padding:10px 14px;font-weight:600;cursor:pointer}
    .go{background:var(--accent);color:#003; } .stop{background:#333;color:var(--fg)}
    .sel, .num {background:#111;color:var(--fg);border:1px solid #222;border-radius:10px;padding:8px}
    .row{display:grid;gap:12px;grid-template-columns:1fr;align-items:start}
    @media(min-width:900px){ .row{grid-template-columns:1fr 1fr} }
    .panel{background:#101317;border:1px solid #1b2129;border-radius:16px;padding:12px}
    .head{display:flex;justify-content:space-between;align-items:center;margin-bottom:8px;color:var(--muted)}
    .meter{font-weight:700}
    .err{color:var(--err);font-weight:700}
    canvas,video{width:100%;height:auto;border-radius:12px;background:#000}
    .log{white-space:pre-wrap;font-family:ui-monospace,Consolas,monospace;background:#0f1216;color:#cfe6ff;border:1px solid #1b2129;padding:8px;border-radius:10px;max-height:34vh;overflow:auto}
    .badge{font-size:0.8rem;color:var(--muted)}
  </style>
  <!-- onnxruntime-web (official CDN) -->
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.19.2/dist/ort.min.js"></script>
</head>
<body>
<div class="wrap">
  <h1>üëÄ CrowdEase ‚Äî Live Person Counter</h1>

  <div class="toolbar">
    <button id="btnStart" class="go">‚ñ∂Ô∏è Start</button>
    <button id="btnStop"  class="stop">‚èπÔ∏è Stop</button>
    <span class="sp"></span>

    <label class="badge">Camera:</label>
    <select id="selCam" class="sel"></select>

    <label class="badge">Facing:</label>
    <select id="selFacing" class="sel">
      <option value="auto" selected>Auto (by facing)</option>
      <option value="user">Front</option>
      <option value="environment">Back</option>
    </select>

    <label class="badge">Threshold:</label>
    <input id="numConf" class="num" type="number" min="0" max="1" step="0.01" value="0.35" style="width:90px" />
  </div>

  <div class="row">
    <div class="panel">
      <div class="head"><div>Camera</div><div id="lblCam" class="badge">idle</div></div>
      <video id="video" playsinline muted></video>
    </div>

    <div class="panel">
      <div class="head">
        <div>Detections (<span id="lblModel">model failed</span>)</div>
        <div class="meter">People: <span id="lblCount">0</span></div>
      </div>
      <canvas id="canvas"></canvas>
    </div>
  </div>

  <p class="badge">Model: <span id="lblModelName">YOLOv8n (ONNX)</span></p>
  <div class="panel">
    <div class="head"><div>Console</div><div class="badge">Open DevTools ‚Üí Console for the same logs.</div></div>
    <div id="log" class="log"></div>
  </div>
</div>

<script>
/* -----------------------------------------------------------
   ‚úÖ ZERO 404/401 GUARANTEE:
   1) Local file at /models/yolov8n.onnx (recommended)
   2) Public jsDelivr fallback (replace USER/REPO/BRANCH below)
----------------------------------------------------------- */
const MODEL_URLS = [
  // Local (served by your Vercel static site ‚Äî place the file here)
  "/models/yolov8n.onnx?v=2",

  // Public CDN fallback (no auth, works on mobile/desktop):
  // üëâ Replace USER, REPO, BRANCH to your repo values (case-sensitive!)
  "https://cdn.jsdelivr.net/gh/IlllIllIIIlIlIll/TransJakarta-CCTV-Simulation@main/models/yolov8n.onnx"
];

// Person class index in COCO for YOLOv8 = 0
const PERSON_CLASS = 0;

// Minimal logging
const elLog = document.getElementById('log');
const log = (...a) => { const s=a.join(' '); console.log(s); elLog.textContent += s + "\n"; elLog.scrollTop = elLog.scrollHeight; };

// UI refs
const elStart  = document.getElementById('btnStart');
const elStop   = document.getElementById('btnStop');
const elVid    = document.getElementById('video');
const elCan    = document.getElementById('canvas');
const elLblCam = document.getElementById('lblCam');
const elLblCnt = document.getElementById('lblCount');
const elLblModel = document.getElementById('lblModel');
const elLblModelName = document.getElementById('lblModelName');
const elSelCam = document.getElementById('selCam');
const elFacing = document.getElementById('selFacing');
const elNumConf = document.getElementById('numConf');

let stream = null;
let running = false;
let session = null;
let modelLoaded = false;
let rafId = null;

// -------------------- Model loader with fallbacks --------------------
async function fetchBinary(url) {
  log("[INFO] GET", url);
  const r = await fetch(url, { mode: 'cors', cache: 'no-cache' });
  if (!r.ok) throw new Error(`HTTP ${r.status}`);
  const buf = await r.arrayBuffer();
  if (buf.byteLength < 1024) throw new Error(`Tiny response (${buf.byteLength} bytes)`);
  return buf;
}

async function loadModel() {
  elLblModel.textContent = "loading‚Ä¶";
  const opts = { executionProviders: ['wasm'], graphOptimizationLevel: 'all' };

  for (const url of MODEL_URLS) {
    try {
      log("[INFO] loading ONNX model‚Ä¶", url);
      const bin = await fetchBinary(url);
      session = await ort.InferenceSession.create(bin, opts);
      modelLoaded = true;
      elLblModel.textContent = "ready";
      elLblModelName.textContent = "YOLOv8n (ONNX)";
      log("[INFO] model ready:", url, `(bytes=${bin.byteLength})`);
      return;
    } catch (err) {
      log("[WARN] model load failed:", err.message || err);
    }
  }
  elLblModel.textContent = "model failed";
  throw new Error("All model URLs failed to load.");
}

// -------------------- Camera helpers --------------------
async function listCameras() {
  const devices = await navigator.mediaDevices.enumerateDevices();
  const cams = devices.filter(d => d.kind === 'videoinput');
  elSelCam.innerHTML = "";
  cams.forEach((c, i) => {
    const o = document.createElement('option');
    o.value = c.deviceId; o.textContent = c.label || `camera ${i+1}`;
    elSelCam.appendChild(o);
  });
  return cams;
}

async function openCamera() {
  if (stream) { stream.getTracks().forEach(t=>t.stop()); stream = null; }
  const facing = elFacing.value; // 'auto'|'user'|'environment'
  const deviceId = elSelCam.value || undefined;

  const constraints = {
    audio: false,
    video: deviceId ? { deviceId: { exact: deviceId } }
                    : (facing === 'auto' ? { facingMode: { ideal: 'environment' } }
                                         : { facingMode: { exact: facing } })
  };

  stream = await navigator.mediaDevices.getUserMedia(constraints);
  elVid.srcObject = stream;
  await elVid.play();
  elLblCam.textContent = `${deviceId ? 'device' : facing} @ ${elVid.videoWidth}√ó${elVid.videoHeight}`;
  elCan.width = elVid.videoWidth; elCan.height = elVid.videoHeight;
}

// -------------------- YOLOv8 post-process (simplified) --------------------
function sigmoid(x){return 1/(1+Math.exp(-x))}
function iou(a,b){const x1=Math.max(a[0],b[0]),y1=Math.max(a[1],b[1]),x2=Math.min(a[2],b[2]),y2=Math.min(a[3],b[3]);const w=Math.max(0,x2-x1),h=Math.max(0,y2-y1);const inter=w*h;const A=(a[2]-a[0])*(a[3]-a[1]);const B=(b[2]-b[0])*(b[3]-b[1]);return inter/(A+B-inter+1e-6)}

function nms(boxes, scores, iouTh=0.45, topK=300){
  const idx = scores.map((s,i)=>[s,i]).sort((a,b)=>b[0]-a[0]).map(x=>x[1]);
  const keep = [];
  for(const i of idx){
    const bi = boxes[i]; let ok = true;
    for(const j of keep){ if(iou(bi, boxes[j])>iouTh){ ok=false; break; } }
    if(ok){ keep.push(i); if(keep.length>=topK) break; }
  }
  return keep;
}

// This assumes YOLOv8n standard ONNX export with output [N,84] (x,y,w,h + 80 class logits)
function decodeYolo(raw, confTh, imgW, imgH){
  // raw: Float32Array (N*84)
  const stride = 84, n = raw.length/stride;
  const boxes=[], scores=[];
  for(let i=0;i<n;i++){
    const base = i*stride;
    const x=raw[base+0], y=raw[base+1], w=raw[base+2], h=raw[base+3];
    // convert center xywh -> xyxy
    const x1 = x - w/2, y1 = y - h/2, x2 = x + w/2, y2 = y + h/2;

    // confidence for "person" class (index 4 + PERSON_CLASS)
    const clsLogit = raw[base + 4 + PERSON_CLASS];
    const p = sigmoid(clsLogit);
    if (p >= confTh){
      boxes.push([x1, y1, x2, y2]);
      scores.push(p);
    }
  }
  // clamp & scale to canvas size if needed (assumes model is pre-scaled to image)
  const keep = nms(boxes, scores, 0.45, 300);
  return keep.map(i => ({box:boxes[i], score:scores[i]}));
}

// -------------------- Inference loop --------------------
async function loop(){
  if(!running) return;
  const t0 = performance.now();

  // draw frame to canvas
  const ctx = elCan.getContext('2d');
  ctx.drawImage(elVid, 0, 0, elCan.width, elCan.height);
  const imgData = ctx.getImageData(0,0,elCan.width, elCan.height);

  // Prepare input tensor (NHWC ‚Üí NCHW, normalized 0..1).
  const W = elCan.width, H = elCan.height;
  const N = W*H;
  const chw = new Float32Array(3*N);
  for(let i=0,j=0;i<N;i++,j+=4){
    const r=imgData.data[j]/255, g=imgData.data[j+1]/255, b=imgData.data[j+2]/255;
    chw[i] = r; chw[i+N] = g; chw[i+2*N] = b;
  }
  const input = new ort.Tensor('float32', chw, [1,3,H,W]);

  // Run ONNX
  const feeds = { images: input };
  let out = await session.run(feeds);

  // Find the first Float32 output
  const key = Object.keys(out).find(k => out[k].data instanceof Float32Array);
  const raw = out[key].data; // Float32Array (N*84)
  const confTh = Math.max(0, Math.min(1, Number(elNumConf.value) || 0.35));

  const dets = decodeYolo(raw, confTh, W, H);

  // draw boxes
  ctx.lineWidth = 2; ctx.strokeStyle = "#39f773"; ctx.fillStyle="rgba(0,0,0,.35)";
  dets.forEach(d=>{
    const [x1,y1,x2,y2]=d.box;
    ctx.fillRect(x1,y1, (x2-x1), 22);
    ctx.strokeRect(x1,y1, (x2-x1), (y2-y1));
    ctx.fillStyle="#fff"; ctx.font="14px ui-monospace";
    ctx.fillText(d.score.toFixed(2), x1+6, y1+16);
    ctx.fillStyle="rgba(0,0,0,.35)";
  });

  elLblCnt.textContent = String(dets.length);

  const t1 = performance.now();
  if(((t1-t0)|0) > 50) log(`[INFO] frame ${(t1-t0).toFixed(1)} ms`);
  rafId = requestAnimationFrame(loop);
}

// -------------------- Lifecycle --------------------
async function start(){
  try{
    elLblCnt.textContent="0";
    if(!modelLoaded) await loadModel();
    await listCameras();
    await openCamera();
    running = true; loop();
  }catch(e){
    log("[ERROR] start failed:", e.message || e);
    elLblModel.textContent = "model failed";
  }
}
function stop(){
  running = false;
  if(rafId) cancelAnimationFrame(rafId), rafId=null;
  if(stream) stream.getTracks().forEach(t=>t.stop()), stream=null;
  elLblCam.textContent = "stopped";
}

elStart.addEventListener('click', start);
elStop .addEventListener('click', stop);
elFacing.addEventListener('change', async ()=>{
  if(!stream) return;
  await openCamera();
});
elSelCam.addEventListener('change', async ()=>{
  if(!stream) return;
  await openCamera();
});

// Autopopulate devices (no camera until user clicks Start due to permissions)
navigator.mediaDevices?.enumerateDevices?.().then(()=>listCameras()).catch(()=>{});

log("[INFO] App ready. Click Start.");
</script>
</body>
</html>
